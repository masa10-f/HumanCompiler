#!/usr/bin/env python3
"""
Simple Local Backup System for TaskAgent
ãƒ­ãƒ¼ã‚«ãƒ«ã‚µãƒ¼ãƒãƒ¼å‘ã‘ã®è»½é‡ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚·ã‚¹ãƒ†ãƒ 
"""

import asyncio
import concurrent.futures
import json
import logging
import os
import re
import shutil
import threading
from datetime import datetime, UTC
from pathlib import Path
from typing import Optional

from taskagent_api.safe_migration import DataBackupManager

logger = logging.getLogger(__name__)


class BackupError(Exception):
    """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ“ä½œã«é–¢ã™ã‚‹ã‚«ã‚¹ã‚¿ãƒ ä¾‹å¤–"""

    pass


class DiskSpaceError(BackupError):
    """ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³ã‚¨ãƒ©ãƒ¼"""

    pass


class BackupValidationError(BackupError):
    """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼ã‚¨ãƒ©ãƒ¼"""

    pass


class BackupConfig:
    """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¨­å®šç®¡ç†"""

    def __init__(self):
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ä»˜ãï¼‰
        self.backup_dir = os.getenv("BACKUP_DIR", "backups")
        self.daily_retention_days = int(os.getenv("BACKUP_DAILY_RETENTION", "7"))
        self.weekly_retention_days = int(os.getenv("BACKUP_WEEKLY_RETENTION", "28"))
        self.min_disk_space_mb = int(os.getenv("BACKUP_MIN_DISK_MB", "100"))
        self.enable_audit_log = os.getenv("BACKUP_AUDIT_LOG", "true").lower() == "true"
        self.max_worker_threads = int(os.getenv("BACKUP_MAX_WORKERS", "2"))

        # ãƒ•ã‚¡ã‚¤ãƒ«æ¨©é™ã®ãƒ­ãƒã‚¹ãƒˆãªãƒ‘ãƒ¼ã‚¹
        try:
            self.file_permissions = int(
                os.getenv("BACKUP_FILE_PERMISSIONS", "0o600"), 8
            )
        except ValueError:
            logger.warning("Invalid file permissions, using default 0o600")
            self.file_permissions = 0o600


class SimpleBackupScheduler:
    """ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ­ãƒ¼ã‚«ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼"""

    def __init__(self, backup_dir: str | None = None):
        self.config = BackupConfig()
        self.backup_dir = Path(backup_dir or self.config.backup_dir)
        self.backup_manager = DataBackupManager(str(self.backup_dir))

        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆã¨æ¨©é™è¨­å®š
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        try:
            self.backup_dir.chmod(0o700)  # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯700æ¨©é™
        except OSError as e:
            logger.warning(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ¨©é™è¨­å®šå¤±æ•—: {e}")

    def _check_disk_space(self, required_mb: int | None = None) -> bool:
        """ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ãƒã‚§ãƒƒã‚¯"""
        try:
            stat = shutil.disk_usage(self.backup_dir)
            free_mb = stat.free // (1024 * 1024)
            min_required = required_mb or self.config.min_disk_space_mb

            if free_mb < min_required:
                raise DiskSpaceError(
                    f"ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³: {free_mb}MB < {min_required}MB required"
                )
            return True
        except Exception as e:
            logger.error(f"ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ãƒã‚§ãƒƒã‚¯å¤±æ•—: {e}")
            if isinstance(e, DiskSpaceError):
                raise
            return False

    def _set_file_permissions(self, file_path: Path):
        """ãƒ•ã‚¡ã‚¤ãƒ«æ¨©é™ã®è¨­å®š"""
        try:
            file_path.chmod(self.config.file_permissions)
        except OSError as e:
            logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«æ¨©é™è¨­å®šå¤±æ•— {file_path}: {e}")

    def _sanitize_audit_data(self, data: dict) -> dict:
        """ç›£æŸ»ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ‹ã‚¿ã‚¤ã‚ºï¼ˆãƒ­ã‚°ã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–ï¼‰"""

        def sanitize_value(value):
            if isinstance(value, str):
                # åˆ¶å¾¡æ–‡å­—ã¨æ”¹è¡Œæ–‡å­—ã‚’é™¤å»
                return re.sub(r"[\x00-\x1f\x7f-\x9f\n\r]", "", value)[:1000]
            elif isinstance(value, dict):
                return {k: sanitize_value(v) for k, v in value.items()}
            elif isinstance(value, list):
                return [sanitize_value(v) for v in value]
            else:
                return value

        return sanitize_value(data)

    def _audit_log(self, action: str, details: dict):
        """ç›£æŸ»ãƒ­ã‚°ã®è¨˜éŒ²ï¼ˆã‚»ã‚­ãƒ¥ã‚¢ç‰ˆï¼‰"""
        if not self.config.enable_audit_log:
            return

        audit_file = self.backup_dir / "audit.log"

        # ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ‹ã‚¿ã‚¤ã‚º
        sanitized_details = self._sanitize_audit_data(details)

        log_entry = {
            "timestamp": datetime.now(UTC).isoformat(),
            "action": action,
            "details": sanitized_details,
        }

        try:
            with open(audit_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
        except Exception as e:
            logger.warning(f"ç›£æŸ»ãƒ­ã‚°è¨˜éŒ²å¤±æ•—: {e}")

    def _validate_backup(self, backup_path: str) -> bool:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯"""
        try:
            with open(backup_path, encoding="utf-8") as f:
                data = json.load(f)

            # å¿…é ˆã‚­ãƒ¼ã®å­˜åœ¨ç¢ºèª
            required_keys = ["users", "projects", "goals", "tasks", "metadata"]
            if not all(key in data for key in required_keys):
                return False

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
            metadata = data.get("metadata", {})
            if "created_at" not in metadata or "total_records" not in metadata:
                return False

            # ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã®ä¸€è‡´ç¢ºèª
            total_records = metadata.get("total_records", {})
            for table in ["users", "projects", "goals", "tasks"]:
                actual_count = len(data.get(table, []))
                expected_count = total_records.get(table, 0)
                if actual_count != expected_count:
                    logger.warning(
                        f"Record count mismatch for {table}: {actual_count} != {expected_count}"
                    )
                    return False

            return True

        except Exception as e:
            logger.error(f"Backup validation failed: {e}")
            return False

    async def _remove_backup_file(self, file_path: Path) -> bool:
        """å˜ä¸€ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤ï¼ˆæŠ½å‡ºã•ã‚ŒãŸãƒ¡ã‚½ãƒƒãƒ‰ï¼‰"""
        try:
            loop = asyncio.get_event_loop()
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                await loop.run_in_executor(executor, file_path.unlink)
            logger.info(f"ğŸ—‘ï¸ Removed old backup: {file_path.name}")
            return True
        except Exception as e:
            logger.warning(f"Failed to remove {file_path}: {e}")
            return False

    async def create_daily_backup(self) -> str:
        """æ—¥æ¬¡ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆï¼ˆéåŒæœŸç‰ˆï¼‰"""
        timestamp = datetime.now(UTC).strftime("%Y%m%d_%H%M%S")
        backup_name = f"daily_backup_{timestamp}"

        try:
            # ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ãƒã‚§ãƒƒã‚¯
            self._check_disk_space()

            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆï¼ˆåˆ¶é™ã•ã‚ŒãŸã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ã§å®Ÿè¡Œï¼‰
            loop = asyncio.get_event_loop()
            with concurrent.futures.ThreadPoolExecutor(
                max_workers=self.config.max_worker_threads
            ) as executor:
                backup_path = await loop.run_in_executor(
                    executor, self.backup_manager.create_backup, backup_name
                )

            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ¤œè¨¼
            if not self._validate_backup(backup_path):
                raise BackupValidationError(f"Backup validation failed: {backup_path}")

            # ãƒ•ã‚¡ã‚¤ãƒ«æ¨©é™è¨­å®š
            self._set_file_permissions(Path(backup_path))

            # ç›£æŸ»ãƒ­ã‚°
            self._audit_log(
                "daily_backup_created",
                {
                    "path": backup_path,
                    "size": Path(backup_path).stat().st_size,
                    "validated": True,
                },
            )

            logger.info(f"âœ… Daily backup created and validated: {backup_path}")

            # å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’éåŒæœŸã§ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            await self._cleanup_old_backups_async(days=self.config.daily_retention_days)

            return backup_path

        except DiskSpaceError as e:
            logger.error(f"âŒ Backup failed due to disk space: {e}")
            self._audit_log(
                "daily_backup_failed", {"error": str(e), "error_type": "disk_space"}
            )
            raise
        except BackupValidationError as e:
            logger.error(f"âŒ Backup validation failed: {e}")
            self._audit_log(
                "daily_backup_failed", {"error": str(e), "error_type": "validation"}
            )
            raise
        except OSError as e:
            logger.error(f"âŒ IO error during backup: {e}")
            self._audit_log(
                "daily_backup_failed", {"error": str(e), "error_type": "io"}
            )
            raise BackupError(f"IO operation failed: {e}")
        except Exception as e:
            logger.error(f"âŒ Unexpected error during backup: {e}")
            self._audit_log(
                "daily_backup_failed", {"error": str(e), "error_type": "unexpected"}
            )
            raise BackupError(f"Backup operation failed: {e}")

    async def create_weekly_backup(self) -> str:
        """é€±æ¬¡ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆï¼ˆéåŒæœŸç‰ˆï¼‰"""
        timestamp = datetime.now(UTC).strftime("%Y%m%d_%H%M%S")
        backup_name = f"weekly_backup_{timestamp}"

        try:
            # ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ãƒã‚§ãƒƒã‚¯
            self._check_disk_space()

            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
            loop = asyncio.get_event_loop()
            with concurrent.futures.ThreadPoolExecutor(
                max_workers=self.config.max_worker_threads
            ) as executor:
                backup_path = await loop.run_in_executor(
                    executor, self.backup_manager.create_backup, backup_name
                )

            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ¤œè¨¼
            if not self._validate_backup(backup_path):
                raise BackupValidationError(
                    f"Weekly backup validation failed: {backup_path}"
                )

            # ãƒ•ã‚¡ã‚¤ãƒ«æ¨©é™è¨­å®š
            self._set_file_permissions(Path(backup_path))

            # ç›£æŸ»ãƒ­ã‚°
            self._audit_log(
                "weekly_backup_created",
                {
                    "path": backup_path,
                    "size": Path(backup_path).stat().st_size,
                    "validated": True,
                },
            )

            logger.info(f"âœ… Weekly backup created and validated: {backup_path}")

            # å¤ã„é€±æ¬¡ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’éåŒæœŸã§ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            await self._cleanup_old_backups_async(
                prefix="weekly_backup_", days=self.config.weekly_retention_days
            )

            return backup_path

        except DiskSpaceError as e:
            logger.error(f"âŒ Weekly backup failed due to disk space: {e}")
            self._audit_log(
                "weekly_backup_failed", {"error": str(e), "error_type": "disk_space"}
            )
            raise
        except BackupValidationError as e:
            logger.error(f"âŒ Weekly backup validation failed: {e}")
            self._audit_log(
                "weekly_backup_failed", {"error": str(e), "error_type": "validation"}
            )
            raise
        except Exception as e:
            logger.error(f"âŒ Weekly backup failed: {e}")
            self._audit_log(
                "weekly_backup_failed", {"error": str(e), "error_type": "unexpected"}
            )
            raise BackupError(f"Weekly backup failed: {e}")

    async def _cleanup_old_backups_async(self, days: int = 7, prefix: str = ""):
        """å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’éåŒæœŸã§å‰Šé™¤"""
        if not self.backup_dir.exists():
            return

        cutoff_date = datetime.now(UTC).timestamp() - (days * 24 * 3600)
        removed_count = 0
        failed_count = 0

        # å‰Šé™¤å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã®åé›†
        files_to_remove = []
        for backup_file in self.backup_dir.glob(f"{prefix}*.json"):
            if backup_file.stat().st_mtime < cutoff_date:
                files_to_remove.append(backup_file)

        # ä¸¦åˆ—å‰Šé™¤å®Ÿè¡Œ
        if files_to_remove:
            tasks = [
                self._remove_backup_file(file_path) for file_path in files_to_remove
            ]
            results = await asyncio.gather(*tasks)
            removed_count = sum(1 for r in results if r)
            failed_count = sum(1 for r in results if not r)

        if removed_count > 0:
            logger.info(f"âœ… Cleaned up {removed_count} old backup files")
            self._audit_log(
                "cleanup_completed",
                {"removed": removed_count, "failed": failed_count, "prefix": prefix},
            )

    def get_backup_status(self) -> dict:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚·ã‚¹ãƒ†ãƒ ã®çŠ¶æ…‹å–å¾—"""
        try:
            backup_files = list(self.backup_dir.glob("*.json"))
            backup_files = [f for f in backup_files if f.name != "audit.log"]

            total_size = sum(f.stat().st_size for f in backup_files)

            # æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®æƒ…å ±
            latest_backup = None
            if backup_files:
                latest_file = max(backup_files, key=lambda f: f.stat().st_mtime)
                latest_backup = {
                    "name": latest_file.name,
                    "created_at": datetime.fromtimestamp(
                        latest_file.stat().st_mtime, UTC
                    ).isoformat(),
                    "size": latest_file.stat().st_size,
                    "validated": self._validate_backup(str(latest_file)),
                }

            # ãƒ‡ã‚£ã‚¹ã‚¯æƒ…å ±
            disk_usage = shutil.disk_usage(self.backup_dir)

            return {
                "status": "healthy" if backup_files else "no_backups",
                "total_backups": len(backup_files),
                "total_size_bytes": total_size,
                "latest_backup": latest_backup,
                "disk_usage": {
                    "free_mb": disk_usage.free // (1024 * 1024),
                    "total_mb": disk_usage.total // (1024 * 1024),
                    "used_mb": (disk_usage.total - disk_usage.free) // (1024 * 1024),
                },
                "backup_directory": str(self.backup_dir),
                "config": {
                    "daily_retention_days": self.config.daily_retention_days,
                    "weekly_retention_days": self.config.weekly_retention_days,
                    "min_disk_space_mb": self.config.min_disk_space_mb,
                },
            }
        except Exception as e:
            logger.error(f"Failed to get backup status: {e}")
            return {"status": "error", "error": str(e)}

    # åŒæœŸç‰ˆãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰
    def create_daily_backup_sync(self) -> str:
        """æ—¥æ¬¡ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆï¼ˆåŒæœŸç‰ˆï¼‰"""
        return asyncio.run(self.create_daily_backup())

    def create_weekly_backup_sync(self) -> str:
        """é€±æ¬¡ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆï¼ˆåŒæœŸç‰ˆï¼‰"""
        return asyncio.run(self.create_weekly_backup())


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç®¡ç†ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ï¼‰
_backup_scheduler = None
_lock = threading.Lock()


def get_backup_scheduler() -> SimpleBackupScheduler:
    """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ï¼‰"""
    global _backup_scheduler
    if _backup_scheduler is None:
        with _lock:
            if _backup_scheduler is None:
                _backup_scheduler = SimpleBackupScheduler()
    return _backup_scheduler


def create_manual_backup() -> str:
    """æ‰‹å‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆï¼ˆcronç”¨ï¼‰"""
    scheduler = get_backup_scheduler()
    return scheduler.create_daily_backup_sync()


if __name__ == "__main__":
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å®Ÿè¡Œæ™‚ã¯æ‰‹å‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
    import sys

    logging.basicConfig(level=logging.INFO)

    try:
        backup_path = create_manual_backup()
        print(f"Backup created: {backup_path}")
        sys.exit(0)
    except Exception as e:
        print(f"Backup failed: {e}")
        sys.exit(1)
